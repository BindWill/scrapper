# Documentation Scraper Setup Guide
What This Does
This tool scrapes and saves documentation from websites into structured JSON files.
Prerequisites
Install Node.js (LTS version)
Install Git
A code editor like Visual Studio Code

## Setup Steps ğŸš€

1. Get the Code
git clone [your-repository-url]
cd [repository-name]

2. Install Dependencies
npm install

3. Run the Scraper
Based on the source code (src/index.ts), you run it like this:
npm start -- https://your-documentation-url.com

## Configuration Options ğŸ› ï¸
You can modify these options when creating the scraper (defaults shown):
const scraper = new DocumentScraper({
    maxDepth: 3,          // How deep to follow links
    checkSublinks: true,  // Check if links are working
    saveToFile: true,     // Save results to file
    outputDir: 'scraped-data',
    timeout: 60000        // 60 seconds timeout
});

## Output Location ğŸ“
Scraped data is automatically saved to the scraped-data folder
Files are named after the domain being scraped
Data is saved in JSON format

## What Gets Scraped ğŸ“
The scraper will collect:
Page titles,
Summaries,
Full text content,
Code blocks,
Section headers,
Metadata,
Sublinks (if enabled)
